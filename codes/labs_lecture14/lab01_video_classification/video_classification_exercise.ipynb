{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 : Video Classification - exercise\n",
    "\n",
    "The goal is to implement three architectures (late fusion 2D-CNN, early fusion 2D-CNN, 3D-CNN) that classifies video sequences.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS4243_codes/codes/labs_lecture14/lab01_video_classification'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device= torch.device(\"cuda\")\n",
    "device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "#from torchvision.transforms import ToTensor\n",
    "#from PIL import Image\n",
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global constants\n",
    "# im_size = image size  \n",
    "# nb_action_classes = number of action classes\n",
    "# nb_steps = number of time steps\n",
    "# batch_size = nb_action_classes * nb_steps\n",
    "# radius_actions = radius of the circular movement \n",
    "im_size = 28 \n",
    "nb_action_classes = 3\n",
    "nb_steps = 10\n",
    "batch_size = nb_action_classes\n",
    "radius_actions = 6\n",
    "\n",
    "# Function that generate a batch of training data\n",
    "def generate_batch_data(im_size, batch_size, nb_action_classes, radius_actions, nb_steps):\n",
    "    batch_videos = torch.zeros(nb_action_classes,nb_steps,im_size,im_size)\n",
    "    batch_labels = torch.zeros(nb_action_classes)\n",
    "    \n",
    "    offset = radius_actions\n",
    "    coord_objects = torch.LongTensor(2).random_(-offset,offset)\n",
    "    \n",
    "    # class action 0\n",
    "    action_class = 0\n",
    "    video = torch.zeros(nb_steps,im_size,im_size) \n",
    "    for k in range(nb_steps):        \n",
    "        y = im_size//2 + (radius_actions* torch.sin(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[1]\n",
    "        x = im_size//2 + (radius_actions* torch.cos(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[0]\n",
    "        video[k,y,x] = 1.0\n",
    "    batch_videos[action_class,:,:,:] = video\n",
    "    batch_labels[action_class] = action_class\n",
    "    # class action 1\n",
    "    action_class = 1\n",
    "    video = torch.zeros(nb_steps,im_size,im_size) \n",
    "    for k in range(nb_steps//2+1):        \n",
    "        y = im_size//2 + (radius_actions* torch.sin(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[1]\n",
    "        x = im_size//2 + (radius_actions* torch.cos(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[0]\n",
    "        video[k,y,x] = 1.0\n",
    "    for k in range(nb_steps-1,nb_steps//2,-1):        \n",
    "        y = im_size//2 + (radius_actions* torch.sin(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[1]\n",
    "        x = im_size//2 + (radius_actions* torch.cos(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[0]\n",
    "        video[nb_steps//2+nb_steps-k,y,x] = 1.0\n",
    "    batch_videos[action_class,:,:,:] = video\n",
    "    batch_labels[action_class] = action_class   \n",
    "    # class action 2\n",
    "    action_class = 2\n",
    "    video = torch.zeros(nb_steps,im_size,im_size) \n",
    "    for k in range(nb_steps//2,-1,-1):  \n",
    "        y = im_size//2 + (radius_actions* torch.sin(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[1]\n",
    "        x = im_size//2 + (radius_actions* torch.cos(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[0]\n",
    "        video[nb_steps//2-k,y,x] = 1.0\n",
    "    for k in range(nb_steps//2+1,nb_steps):        \n",
    "        y = im_size//2 + (radius_actions* torch.sin(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[1]\n",
    "        x = im_size//2 + (radius_actions* torch.cos(torch.tensor(k*2*3.1415/nb_steps))).long() + coord_objects[0]\n",
    "        video[k,y,x] = 1.0\n",
    "    batch_videos[action_class,:,:,:] = video\n",
    "    batch_labels[action_class] = action_class  \n",
    "    return batch_videos, batch_labels\n",
    "\n",
    "# Plot a mini-batch of images\n",
    "batch_videos, batch_labels = generate_batch_data(im_size, batch_size, nb_action_classes, radius_actions, nb_steps)\n",
    "print(batch_videos.size())\n",
    "print(batch_labels.size())\n",
    "for a in range(nb_action_classes):\n",
    "    fig = plt.figure(1,figsize=(7,7))\n",
    "    plt.show(block=False)\n",
    "    for k in range(nb_steps):\n",
    "        plt.imshow(batch_videos[a,k,:,:], cmap='gray')\n",
    "        plt.title('Action class : ' + str(a) )\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(0.15)\n",
    "    plt.close()\n",
    "fig = plt.figure(1,figsize=(7,7))\n",
    "plt.show(block=False)\n",
    "sum_images = torch.zeros(im_size,im_size)\n",
    "for t in range(nb_steps):\n",
    "    sum_images = sum_images + batch_videos[a,t,:,:]\n",
    "plt.imshow(sum_images, cmap='gray')\n",
    "plt.title('Sum of temporal images')\n",
    "fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the video classification network architecture\n",
    "class videoNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, type_videocnn):\n",
    "        super(videoNN, self).__init__()\n",
    "        # Hyper-parameters\n",
    "        self.type_videocnn = type_videocnn\n",
    "        hidden_dim = 50\n",
    "        # Single-frame/late fusion 2D-CNN\n",
    "        if type_videocnn == 'late_fusion':\n",
    "            # conv layer\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "            # Classification layer from flattened conv features\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "        # Early fusion 2D-CNN\n",
    "        if type_videocnn == 'early_fusion':\n",
    "            # conv layer\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "            # Classification layer from flattened conv features\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "        if type_videocnn == '3d_cnn':\n",
    "            # conv layer\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "            # Classification layer from flattened conv features\n",
    "            # COMPLETE HERE\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # Single-frame/late fusion 2D-CNN\n",
    "        if self.type_videocnn == 'late_fusion':\n",
    "            # Conv2D for each time frame independently\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "            # Classification layer\n",
    "            \n",
    "        # Early fusion 2D-CNN\n",
    "        if self.type_videocnn == 'early_fusion':\n",
    "            # Conv2D for concatenated frames\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "            # Classification layer\n",
    "            \n",
    "        # 3D-CNN\n",
    "        if self.type_videocnn == '3d_cnn':\n",
    "            # Conv3D for volume time x space\n",
    "            # COMPLETE HERE\n",
    "            \n",
    "            # Classification layer\n",
    "            \n",
    "        return scores_action_class\n",
    "    \n",
    "# Instantiate the network\n",
    "type_videocnn = 'late_fusion'\n",
    "#type_videocnn = 'early_fusion'\n",
    "#type_videocnn = '3d_cnn'\n",
    "net = videoNN(type_videocnn)\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net) \n",
    "\n",
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "batch_videos, batch_labels = generate_batch_data(im_size, batch_size, nb_action_classes, radius_actions, nb_steps)\n",
    "print(batch_videos.size())\n",
    "print(batch_labels.size())\n",
    "optimizer.zero_grad()\n",
    "scores_action_class = net(batch_videos) # [batch_size, nb_action_classes] = [3, 3]\n",
    "batch_labels = batch_labels.long() # [batch_size] = [3]\n",
    "# loss\n",
    "loss = nn.CrossEntropyLoss()(scores_action_class, batch_labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "type_videocnn = 'late_fusion'\n",
    "#type_videocnn = 'early_fusion'\n",
    "#type_videocnn = '3d_cnn'\n",
    "print('Architecture : ',type_videocnn)\n",
    "net = videoNN(type_videocnn)\n",
    "net = net.to(device)\n",
    "utils.display_num_param(net) \n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_batch = 25\n",
    "\n",
    "start=time.time()\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for _ in range(nb_batch):\n",
    "        \n",
    "        # FORWARD AND BACKWARD PASS\n",
    "        batch_videos, batch_labels = generate_batch_data(im_size, batch_size, nb_action_classes, radius_actions, nb_steps)\n",
    "        optimizer.zero_grad()\n",
    "        scores_action_class = net(batch_videos) # [batch_size, nb_action_classes] = [3, 3]\n",
    "        batch_labels = batch_labels.long() # [batch_size] = [3]\n",
    "        # loss\n",
    "        loss = nn.CrossEntropyLoss()(scores_action_class, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # COMPUTE STATS\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', init_lr  ,'\\t loss=', total_loss )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test time\n",
    "\n",
    "# generate the batch of 3 actions\n",
    "batch_videos, batch_labels = generate_batch_data(im_size, batch_size, nb_action_classes, radius_actions, nb_steps)\n",
    "\n",
    "# forward pass\n",
    "scores_action_class = net(batch_videos) # [batch_size, nb_action_classes] = [3, 3]\n",
    "\n",
    "# class prediction\n",
    "pred_action_class = torch.argmax(scores_action_class, dim=1) # [batch_size] = [3]\n",
    "\n",
    "print('Predicted action class : ',pred_action_class)\n",
    "print('Label action class     : ',batch_labels.long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
