{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 : Vanilla Faster R-CNN - exercise\n",
    "\n",
    "The goal is to implement the forward pass of the vanilla faster CNN architecture at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS4243_codes/codes/labs_lecture11'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device= torch.device(\"cuda\")\n",
    "device= torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL) # remove warnings\n",
    "\n",
    "# Import 5 object types\n",
    "nb_class_objects = 5\n",
    "objects = torch.zeros(nb_class_objects,7,7)\n",
    "for k in range(nb_class_objects):\n",
    "    objects[k,:,:] = 1-ToTensor()(Image.open('objects/obj'+str(k+1)+'.tif'))[0,:,:]\n",
    "print(objects.size())\n",
    "\n",
    "# Define the bounding boxes w.r.t. object type\n",
    "def box_coord(label, x, y, w):\n",
    "    offset = (w-1)// 2 + 0.5\n",
    "    x = [x-offset, x-offset+w, x-offset+w, x-offset,   x-offset]\n",
    "    y = [y-offset, y-offset,   y-offset+w, y-offset+w, y-offset]\n",
    "    if label==0:\n",
    "        color = 'r'; legend = 'Triangle'\n",
    "    elif label==1:\n",
    "        color = 'b'; legend = 'Cross'\n",
    "    elif label==2:\n",
    "        color = 'g'; legend = 'Star'\n",
    "    elif label==3:\n",
    "        color = 'y'; legend = 'Square'\n",
    "    elif label==4:\n",
    "        color = 'm'; legend = 'Ring'\n",
    "    return x, y, color, legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "# im_size = image size  \n",
    "# ob_size = object size\n",
    "# batch_size = batch size\n",
    "# nb_objects = number of objects in the image\n",
    "# nb_class_objects = number of object classes (we have 5 classes)\n",
    "# hidden_dim = hidden dimension\n",
    "\n",
    "im_size = 28 \n",
    "ob_size = 7\n",
    "offset = (ob_size-1)// 2 \n",
    "batch_size = 2 \n",
    "nb_objects = 3\n",
    "hidden_dim = 64\n",
    "\n",
    "# Function that generate a batch of training data\n",
    "def generate_batch_data(im_size, ob_size, batch_size, nb_objects, nb_class_objects):\n",
    "    batch_images = torch.zeros(batch_size,im_size,im_size)\n",
    "    batch_bboxes = torch.zeros(batch_size,nb_objects,4+1) # 2D-coord, width and height of boxes, and their class\n",
    "    for b in range(batch_size):\n",
    "        image = torch.zeros(im_size,im_size)\n",
    "        bboxes = torch.zeros(nb_objects,4+1)\n",
    "        class_objects = torch.LongTensor(nb_objects).random_(0,nb_class_objects)\n",
    "        offset = (ob_size-1)// 2\n",
    "        coord_objects = torch.LongTensor(nb_objects,2).random_(offset,im_size-offset)\n",
    "        # coord_objects[k,0] = x-coordinate,  coord_objects[k,1] = y-coordinate\n",
    "        for k in range(nb_objects):\n",
    "            image[coord_objects[k,1]-offset:coord_objects[k,1]-offset+ob_size,coord_objects[k,0]-offset:coord_objects[k,0]-offset+ob_size] += objects[class_objects[k],:,:]; image[image>=1.0] = 1.0\n",
    "            bboxes[k,:] = torch.Tensor([coord_objects[k,0],coord_objects[k,1],ob_size,ob_size,class_objects[k]])\n",
    "        batch_images[b,:,:] = image\n",
    "        batch_bboxes[b,:,:] = bboxes\n",
    "    return batch_images, batch_bboxes\n",
    "\n",
    "# Plot a mini-batch of images\n",
    "batch_images, batch_bboxes = generate_batch_data(im_size, ob_size, batch_size, nb_objects, nb_class_objects)\n",
    "print(batch_images.size())\n",
    "print(batch_bboxes.size())\n",
    "for b in range(batch_size):\n",
    "    plt.imshow(batch_images[b,:,:], cmap='gray')\n",
    "    for k in range(nb_objects):\n",
    "        x, y, color, legend = box_coord(batch_bboxes[b,k,4],batch_bboxes[b,k,0],batch_bboxes[b,k,1],ob_size)\n",
    "        plt.plot(x,y,color,label=legend)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the faster-R-CNN architecture\n",
    "batch_size = 2 # for debug\n",
    "\n",
    "class faster_R_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(faster_R_CNN, self).__init__()\n",
    "        # backbone convnet\n",
    "        self.conv1 = nn.Conv2d(1,  hidden_dim,  kernel_size=3, padding=1 ) #  1x28x28 --> hidden_dimx 28x28 \n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1 ) # hidden_dim x28x28 --> hidden_dim x28x28 \n",
    "        # Bbox classes\n",
    "        self.linear_class = nn.Linear(hidden_dim* ob_size**2, nb_class_objects) # hidden_dim x7x7 --> nb_class_objects \n",
    "        # Bbox anchors\n",
    "        self.conv_anch = nn.Conv2d(hidden_dim, 1, kernel_size=ob_size, padding=offset )  # hidden_dim x28x28 --> 1x28x28 \n",
    "        \n",
    "    def forward(self, x, bb, train_flag=True):\n",
    "        # backbone convnet\n",
    "        x = self.conv1(x) # [batch_size, hidden_dim, im_size, im_size] \n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x) # [batch_size,hidden_dim, im_size, im_size]\n",
    "        x = torch.relu(x) \n",
    "        # Bbox anchors\n",
    "        scores_bbox_anch = self.conv_anch(x).squeeze() # [batch_size, im_size, im_size] \n",
    "        # Bbox classes\n",
    "        if train_flag: # forward pass at training time\n",
    "            bbox = []\n",
    "            for b in range(batch_size):\n",
    "                for k in range(nb_objects):\n",
    "                    bbox.append(x[b,:,bb[b,k,1].long()-offset:bb[b,k,1].long()-offset+ob_size,bb[b,k,0].long()-offset:bb[b,k,0].long()-offset+ob_size])\n",
    "            bbox = torch.stack(bbox, dim=0) # [batch_size*nb_objects, hidden_dim, ob_size, ob_size]\n",
    "            bbox = bbox.view(-1, hidden_dim * ob_size**2) # [batch_size*nb_objects, hidden_dim*ob_size*ob_size]\n",
    "            scores_bbox_class = self.linear_class(bbox) # [batch_size*nb_objects, nb_class_objects]\n",
    "        else: # forward pass at test time\n",
    "            batch_bbox = []\n",
    "            for b in range(batch_size):\n",
    "                # compute the coordinates of the top-K bbox anchor scores (K=nb_objects)\n",
    "                # COMPLETE HERE\n",
    "                scores_bbox_anch_b = \n",
    "                idx_y = \n",
    "                idx_x = \n",
    "                # extract the top-K bboxes of size [batch_size, nb_objects, hidden_dim, ob_size, ob_size]\n",
    "                # COMPLETE HERE\n",
    "                bbox = \n",
    "            # compute the class scores of the bbox\n",
    "            # size of tensor scores_bbox_class is [batch_size*nb_objects, nb_class_objects]\n",
    "            # COMPLETE HERE\n",
    "            scores_bbox_class = \n",
    "        return scores_bbox_class, scores_bbox_anch\n",
    "    \n",
    "# Instantiate the network\n",
    "net = faster_R_CNN()\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "utils.display_num_param(net) \n",
    "\n",
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "batch_images, batch_bboxes = generate_batch_data(im_size, ob_size, batch_size, nb_objects, nb_class_objects)\n",
    "optimizer.zero_grad()\n",
    "scores_bbox_class, scores_bbox_anch = net(batch_images.unsqueeze(dim=1), batch_bboxes)\n",
    "# Bbox class loss\n",
    "labels_bbox_class = batch_bboxes[:,:,4].long().view(-1) # [batch_size*nb_objects]\n",
    "loss_class = nn.CrossEntropyLoss()(scores_bbox_class, labels_bbox_class)\n",
    "# Bbox anchors loss\n",
    "labels_bbox_coord = batch_bboxes[:,:,0:2] # [batch_size, nb_objects, 2D]\n",
    "labels_bbox_anch = torch.zeros(batch_size, im_size, im_size) # [batch_size, im_size, im_size]\n",
    "for b in range(batch_size):\n",
    "    for k in range(nb_objects): \n",
    "        labels_bbox_anch[b,labels_bbox_coord[b,k,1].long(),labels_bbox_coord[b,k,0].long()] = 1\n",
    "loss_anch = torch.nn.BCEWithLogitsLoss()(scores_bbox_anch, labels_bbox_anch)\n",
    "loss = loss_class + loss_anch\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "net = faster_R_CNN()\n",
    "net = net.to(device)\n",
    "utils.display_num_param(net) \n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_batch = 10\n",
    "batch_size = 10\n",
    "\n",
    "start=time.time()\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for _ in range(nb_batch):\n",
    "        \n",
    "        # FORWARD AND BACKWARD PASS\n",
    "        batch_images, batch_bboxes = generate_batch_data(im_size, ob_size, batch_size, nb_objects, nb_class_objects)\n",
    "        optimizer.zero_grad()\n",
    "        scores_bbox_class, scores_bbox_anch = net(batch_images.unsqueeze(dim=1), batch_bboxes)\n",
    "        # Bbox class loss\n",
    "        labels_bbox_class = batch_bboxes[:,:,4].long().view(-1) # [batch_size*nb_objects]\n",
    "        loss_class = nn.CrossEntropyLoss()(scores_bbox_class, labels_bbox_class)\n",
    "        # Bbox anchors loss\n",
    "        labels_bbox_coord = batch_bboxes[:,:,0:2] # [batch_size, nb_objects, 2D]\n",
    "        labels_bbox_anch = torch.zeros(batch_size, im_size, im_size) # [batch_size, im_size, im_size]\n",
    "        for b in range(batch_size):\n",
    "            for k in range(nb_objects): \n",
    "                labels_bbox_anch[b,labels_bbox_coord[b,k,1].long(),labels_bbox_coord[b,k,0].long()] = 1\n",
    "        loss_anch = torch.nn.BCEWithLogitsLoss()(scores_bbox_anch, labels_bbox_anch)\n",
    "        loss = loss_class + loss_anch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # COMPUTE STATS\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1        \n",
    "    \n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = (time.time()-start)/60\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', init_lr  ,'\\t loss=', total_loss )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test time\n",
    "\n",
    "# select a batch of 2 images\n",
    "batch_size = 2 \n",
    "\n",
    "# generate the batch of 2 images\n",
    "batch_images, batch_bboxes = generate_batch_data(im_size, ob_size, batch_size, nb_objects, nb_class_objects)\n",
    "\n",
    "# Forward pass during TEST time\n",
    "#  Note the flag \"train_flag=False\"\n",
    "scores_bbox_class, scores_bbox_anch = net(batch_images.unsqueeze(dim=1), batch_bboxes, train_flag=False)\n",
    "         \n",
    "# scores_bbox_class [batch_size*nb_objects, nb_class_objects] \n",
    "bbox_class = torch.argmax(scores_bbox_class, dim=-1) # [batch_size*nb_objects] \n",
    "bbox_class = bbox_class.view(batch_size, nb_objects) # [batch_size, nb_objects] \n",
    "\n",
    "# Ground tructh Bbox anchors\n",
    "labels_bbox_coord = batch_bboxes[:,:,0:2] # [batch_size, nb_objects, 2]\n",
    "labels_bbox_anch = torch.zeros(batch_size, im_size, im_size) # [batch_size, im_size, im_size]\n",
    "for b in range(batch_size):\n",
    "    for k in range(nb_objects): \n",
    "        labels_bbox_anch[b,labels_bbox_coord[b,k,1].long(),labels_bbox_coord[b,k,0].long()] = 1\n",
    "    \n",
    "# Plot the ground truth solution and the predicted solution\n",
    "for b in range(batch_size):\n",
    "    \n",
    "    # Plot ground truth and prediction\n",
    "    plt.imshow(batch_images[b,:,:], cmap='gray')\n",
    "    for k in range(nb_objects):\n",
    "        x, y, color, legend = box_coord(batch_bboxes[b,k,4],batch_bboxes[b,k,0],batch_bboxes[b,k,1],ob_size)\n",
    "        plt.plot(x,y,color,label=legend)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Ground Truth')\n",
    "    plt.show() \n",
    "    # Predicted anchor coordinates\n",
    "    scores_bbox_anch_b = scores_bbox_anch[b,:,:].detach()\n",
    "    scores_bbox_anch_b = scores_bbox_anch_b.view(-1)\n",
    "    _, idx_largest = torch.sort(scores_bbox_anch_b, descending=True) # rank the scores from largest to smallest\n",
    "    idx_largest = idx_largest[:nb_objects] # get the top scores\n",
    "    idx_y = idx_largest//im_size        # compute y-coord of anchor locations\n",
    "    idx_x = idx_largest - idx_y*im_size # compute x-coord of anchor locations\n",
    "    # Predicted classes for the boxes\n",
    "    bbox_class = torch.argmax(scores_bbox_class, dim=-1) # [batch_size*nb_objects]\n",
    "    bbox_class = bbox_class.view(batch_size, nb_objects) # [batch_size, nb_objects]\n",
    "    plt.imshow(batch_images[b,:,:], cmap='gray')\n",
    "    for k in range(nb_objects):\n",
    "        x, y, color, legend = box_coord(bbox_class[b,k],idx_x[k],idx_y[k],ob_size) # Predicted classes\n",
    "        plt.plot(x,y,color,label=legend)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Prediction')\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot ground truth and predicted locations of anchors\n",
    "    plt.imshow(labels_bbox_anch[b,:,:], cmap='gray')\n",
    "    plt.title('Ground Truth locations of anchors')\n",
    "    plt.show() \n",
    "    plt.imshow(scores_bbox_anch[b,:,:].detach(), cmap='gray')\n",
    "    plt.title('Predicted locations of anchors')\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
